{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_normalized = [[normalize(caption) for caption in captions] for captions in captions_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_length_count(captions_list):\n",
    "    length_count = list()\n",
    "    for captions in captions_list:\n",
    "        if len(captions) == 0:\n",
    "            print('NO CAPTION!!')\n",
    "        for caption in captions:\n",
    "            L = len(caption.split(' '))\n",
    "            length_count.append(L)\n",
    "\n",
    "    print(min(length_count), max(length_count), len(length_count))\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.hist(length_count, max(length_count) - min(length_count) + 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 46 24232\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAJCkThkAktQpA0CSOvX/mc9qN1bHfz0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05bb7507b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_length_count(captions_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_captions(captions_list):\n",
    "    filtered_captions_list = []\n",
    "    for captions in captions_list:\n",
    "        filtered_captions = []\n",
    "        for caption in captions:\n",
    "            L = len(caption.split(' '))\n",
    "            if MIN_LENGTH <= L <= MAX_LENGTH:\n",
    "                filtered_captions.append(caption)\n",
    "        filtered_captions_list.append(filtered_captions)\n",
    "    return filtered_captions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 20 23585\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05bbc9f2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "captions_train_filtered = filter_captions(captions_train_normalized)\n",
    "\n",
    "show_length_count(captions_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing words...\n",
      "Indexed 5709 words in output\n"
     ]
    }
   ],
   "source": [
    "output_lang = Lang()\n",
    "\n",
    "print(\"Indexing words...\")\n",
    "for captions in captions_train_filtered:\n",
    "    for caption in captions:\n",
    "        output_lang.index_words(caption)\n",
    "\n",
    "print('Indexed {} words in output'.format(output_lang.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 3538 / 5706 = 0.6200\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 2\n",
    "\n",
    "output_lang.trim(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 23585 pairs to 21672, 0.9189 of total\n"
     ]
    }
   ],
   "source": [
    "captions_train_trimmed = []\n",
    "\n",
    "for captions in captions_train_filtered:\n",
    "    keep_captions = []\n",
    "    for caption in captions:\n",
    "        keep = True\n",
    "\n",
    "        for word in caption.split(' '):\n",
    "            if word not in output_lang.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "\n",
    "        # Remove if pair doesn't match input and output conditions\n",
    "        if keep:\n",
    "            keep_captions.append(caption)\n",
    "    \n",
    "    captions_train_trimmed.append(keep_captions)\n",
    "\n",
    "L1 = sum([len(captions) for captions in captions_train_trimmed])\n",
    "L2 = sum([len(captions) for captions in captions_train_filtered])\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % (L2, L1, L1 / L2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "num_beams = 3\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "    losses = [0]\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "\n",
    "    else:\n",
    "        beams = [[(decoder_input, decoder_context, decoder_hidden)]]\n",
    "        num_finished = 0\n",
    "        \n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "            beams_new = []\n",
    "            losses_new = []\n",
    "            for i, beam in enumerate(beams):\n",
    "                decoder_input, decoder_context, decoder_hidden = beam[-1]\n",
    "                if decoder_input.data[0][0] == EOS_token:\n",
    "                    continue\n",
    "                \n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "            \n",
    "                losses[i] += criterion(decoder_output, target_variable[di])\n",
    "            \n",
    "                # Get most likely word index (highest value) from output\n",
    "                topv, topi = decoder_output.data.topk(num_beams)\n",
    "                for j in range(num_beams):\n",
    "                    ni = topi[0][j]\n",
    "                    decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input\n",
    "                    if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "                    \n",
    "                    beams_new.append(beam + [(decoder_input, decoder_context, decoder_hidden)])\n",
    "                    losses_new.append(losses[i])\n",
    "\n",
    "                    # Stop at end of sentence (not necessary when using known targets)\n",
    "                    if ni == EOS_token:\n",
    "                        num_finished += 1\n",
    "\n",
    "            if len(losses_new) > num_beams:\n",
    "                \n",
    "                        \n",
    "            beams = beams_new\n",
    "            losses = losses_new\n",
    "            \n",
    "            if num_finished >= num_beams:\n",
    "                break\n",
    "    \n",
    "#         print(losses)\n",
    "        loss = min([i[0] for i in losses])\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_train(index, encoder, decoder, max_length=80):\n",
    "    input_variable = X_train[index]\n",
    "    input_variable = Variable(torch.FloatTensor(input_variable))\n",
    "    if USE_CUDA: input_variable = input_variable.cuda()\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(num_features, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "# n_epochs = 100000\n",
    "# plot_every = 200\n",
    "# print_every = 100\n",
    "# save_every = 1000\n",
    "\n",
    "n_epochs = 10\n",
    "plot_every = 2\n",
    "print_every = 1\n",
    "save_every = 10\n",
    "\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f64ce3455fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Run the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Keep track of loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-44abc66fbdf7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/hw2/src/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, last_context, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x 1 x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/hw2/src/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Calculate energies for each encoder output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mattn_energies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/hw2/src/models.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'general'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_addcop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, vector1, vector2)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "import time\n",
    "import random\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    index = random.randrange(num_videos)\n",
    "    input_variable = X_train[index]\n",
    "    input_variable = Variable(torch.FloatTensor(input_variable))\n",
    "    if USE_CUDA: input_variable = input_variable.cuda()\n",
    "    \n",
    "    captions = captions_train_trimmed[index]\n",
    "    num_captions = len(captions)\n",
    "    caption = captions[epoch % num_captions]\n",
    "    target_variable = variable_from_sentence(output_lang, caption)\n",
    "    \n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        with open('./log.txt', 'a') as log_f:\n",
    "            log_f.write(print_summary + '\\n')\n",
    "        \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        output_words, decoder_attn = evaluate_on_train(index, encoder, decoder)\n",
    "        output_sentence = ' '.join(output_words[:-1])\n",
    "        print('Truth:   ', captions[epoch % num_captions])\n",
    "        print('Predict: ', output_sentence)\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "    \n",
    "    if epoch % save_every == 0:\n",
    "        MODEL_DIR = '../models/new/'\n",
    "        torch.save(encoder.state_dict(), MODEL_DIR + './encoder_epoch{}.sd'.format(str(epoch)))\n",
    "        torch.save(decoder.state_dict(), MODEL_DIR + './decoder_epoch{}.sd'.format(str(epoch)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(id_, max_length=80):\n",
    "    index = filenames_test.index(id_ + '.npy')\n",
    "    input_variable = X_test[index]\n",
    "    input_variable = Variable(torch.FloatTensor(input_variable))\n",
    "    if USE_CUDA: input_variable = input_variable.cuda()\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'boy', 'is', 'a']\n"
     ]
    }
   ],
   "source": [
    "def postprocess(words):\n",
    "    for i in range(len(words) - 1, -1, -1):\n",
    "        if words[i] == '.':\n",
    "            continue\n",
    "        if words[i] == 'a' and words[i-1] == 'a':\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    return words[:i+1]\n",
    "\n",
    "print(postprocess(['a', 'boy', 'is', 'a', 'a', '.', '.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is is is . is . is . is\n",
      "a is is . is . is . is\n",
      "a is is is is . is is . is is . is is . is is . is is . is is . is is . is is\n",
      "a is is . is . is . is\n",
      "a is is . is . is\n"
     ]
    }
   ],
   "source": [
    "ids = [\n",
    "    'klteYv1Uv9A_27_33.avi',\n",
    "    '5YJaS2Eswg0_22_26.avi',\n",
    "    'UbmZAe5u5FI_132_141.avi',\n",
    "    'JntMAcTlOF0_50_70.avi',\n",
    "    'tJHUH9tpqPg_113_118.avi',\n",
    "]\n",
    "\n",
    "for id_ in ids:\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(id_)\n",
    "    output_sentence = ' '.join(postprocess(output_words[:-1]))\n",
    "    print(output_sentence)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4ba1f0e83ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_videos_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mid_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilenames_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f68ed125d498>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(id_, max_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilenames_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minput_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with open('output_3.csv', 'w') as f:\n",
    "    for i in range(num_videos_test):\n",
    "        id_ = filenames_test[i][:-4]\n",
    "        output_words, decoder_attn = evaluate(id_)\n",
    "        output_sentence = ' '.join(postprocess(output_words[:-1]))\n",
    "        f.write(id_ + ',' + output_sentence + '\\n')\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "torch.save(encoder, MODEL_DIR + './encoder_z2.sd')\n",
    "torch.save(decoder, MODEL_DIR + './decoder_z2.sd')\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
