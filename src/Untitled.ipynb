{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "~~read test files~~\n",
    "\n",
    "do normalization separately for fbank and mfcc?\n",
    "\n",
    "~~padding~~\n",
    "\n",
    "**Use 48 for training and map after prediction**\n",
    "\n",
    "outliers?\n",
    "\n",
    "adam?\n",
    "\n",
    "use custom metric (edit distance)\n",
    "\n",
    "~~BLSTM~~\n",
    "\n",
    "monitor for modelcheckpoint and earlystopping\n",
    "\n",
    "~~no validation?~~\n",
    "\n",
    "**Add padding as 40th class?**\n",
    "\n",
    "dropout / recurrent dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Bidirectional, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import arrow\n",
    "\n",
    "# GPU usage\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(666)\n",
    "NUM_UNITS = 100\n",
    "DROPOUT = 0.1\n",
    "# RECURRENT_DROPOUT = 0.3\n",
    "FEATURE_LEN = 108\n",
    "# NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 2\n",
    "OPTIMIZER = 'rmsprop'\n",
    "NUM_CLASSES = 39\n",
    "DATA_PATH = '../data/'\n",
    "MODEL_PATH = '../models/'\n",
    "PREDICTION_PATH = '../predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ark(filepath, sentence2frames):\n",
    "    with open(filepath) as train_ark_f:\n",
    "        last_sentence = ''\n",
    "        frames = list()\n",
    "        is_first_line = True\n",
    "        for line in train_ark_f:\n",
    "            line = line.strip().split()\n",
    "            instance_id = line[0]\n",
    "            speaker_id, sentence_id, frame_id = instance_id.split('_')\n",
    "            sentence = '_'.join((speaker_id, sentence_id))\n",
    "            features = list(map(float, line[1:]))\n",
    "\n",
    "            if sentence != last_sentence and not is_first_line:\n",
    "                if last_sentence not in sentence2frames:\n",
    "                    sentence2frames[last_sentence] = frames\n",
    "                else:\n",
    "                    sentence2frames[last_sentence] = np.concatenate(\n",
    "                        (sentence2frames[last_sentence], frames),\n",
    "                        axis=1\n",
    "                    )\n",
    "                frames = list()\n",
    "\n",
    "            last_sentence = sentence\n",
    "            frames.append(features)\n",
    "            is_first_line = False\n",
    "\n",
    "        # process last sentence\n",
    "        if last_sentence not in sentence2frames:\n",
    "            sentence2frames[last_sentence] = frames\n",
    "        else:\n",
    "            sentence2frames[last_sentence] = np.concatenate(\n",
    "                (sentence2frames[last_sentence], frames),\n",
    "                axis=1\n",
    "            )\n",
    "        frames = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53 s, sys: 2.87 s, total: 55.9 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentence2frames = dict()\n",
    "read_ark(DATA_PATH + 'fbank/train.ark', sentence2frames)\n",
    "read_ark(DATA_PATH + 'mfcc/train.ark', sentence2frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696\n",
      "474\n",
      "108\n",
      "3.148541\n"
     ]
    }
   ],
   "source": [
    "# number of training sentences\n",
    "NUM_SENTENCES = len(sentence2frames)\n",
    "print(NUM_SENTENCES)\n",
    "\n",
    "# number of frames in this sentence\n",
    "print(len(sentence2frames['faem0_si1392']))\n",
    "\n",
    "# number of fbank+mfcc features in a frame\n",
    "print(len(sentence2frames['faem0_si1392'][0]))\n",
    "\n",
    "# first feature\n",
    "print(sentence2frames['faem0_si1392'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LEN = 0\n",
    "for s in sentence2frames:\n",
    "    MAX_SENTENCE_LEN = max(MAX_SENTENCE_LEN, len(sentence2frames[s]))\n",
    "print(MAX_SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone48_phone39 = dict()\n",
    "phone39_int39 = dict()\n",
    "int39_phone39 = dict()\n",
    "phone48_char48 = dict()\n",
    "\n",
    "with open(DATA_PATH + 'phones/48_39.map') as phone48_phone39_f:\n",
    "    for line in phone48_phone39_f:\n",
    "        phone48, phone39 = line.strip().split('\\t')\n",
    "        phone48_phone39[phone48] = phone39\n",
    "        if phone39 not in phone39_int39:\n",
    "            int39 = len(phone39_int39)\n",
    "            phone39_int39[phone39] = int39\n",
    "            int39_phone39[int39] = phone39\n",
    "\n",
    "with open(DATA_PATH + '48phone_char.map') as phone48_char48_f:\n",
    "    for line in phone48_char48_f:\n",
    "        phone48, int48, char48 = line.strip().split('\\t')\n",
    "        phone48_char48[phone48] = char48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 16 ms, total: 1.46 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def read_label(filepath, sentence2labels):\n",
    "    with open(filepath) as train_lab_f:\n",
    "        last_sentence = ''\n",
    "        labels = list()\n",
    "        is_first_line = True\n",
    "        \n",
    "        for line in train_lab_f:\n",
    "            instance_id, label = line.strip().split(',')\n",
    "            speaker_id, sentence_id, frame_id = instance_id.split('_')\n",
    "            sentence = '_'.join((speaker_id, sentence_id))\n",
    "            index = phone39_int39[phone48_phone39[label]]\n",
    "\n",
    "            if sentence != last_sentence and not is_first_line:\n",
    "                sentence2labels[last_sentence] = labels\n",
    "                labels = list()\n",
    "\n",
    "            last_sentence = sentence\n",
    "            labels.append(index)\n",
    "            is_first_line = False\n",
    "\n",
    "        # process last sentence\n",
    "        sentence2labels[last_sentence] = labels\n",
    "        labels = list()\n",
    "\n",
    "sentence2labels = dict()\n",
    "read_label(DATA_PATH + 'label/train.lab', sentence2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696\n",
      "474\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# number of training sentences\n",
    "print(len(sentence2labels))\n",
    "\n",
    "# number of frames in this sentence\n",
    "print(len(sentence2labels['faem0_si1392']))\n",
    "\n",
    "# first label\n",
    "print(sentence2labels['faem0_si1392'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 1.41 s, total: 5.88 s\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def one_hot(n, i):\n",
    "    ans = np.zeros(n)\n",
    "    ans[i] = 1\n",
    "    return ans\n",
    "\n",
    "sentences = sorted(sentence2labels)\n",
    "X = np.zeros((NUM_SENTENCES, MAX_SENTENCE_LEN, FEATURE_LEN))\n",
    "y = np.zeros((NUM_SENTENCES, MAX_SENTENCE_LEN, NUM_CLASSES))\n",
    "\n",
    "for i, s in enumerate(sentences):\n",
    "    frames = sentence2frames[s]\n",
    "    X[i, :len(frames)] = frames\n",
    "    y[i, :len(frames)] = np.stack([one_hot(NUM_CLASSES, l) for l in sentence2labels[s]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3696, 777, 108)\n",
      "(3696, 777, 39)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_9 (Bidirection (None, 777, 200)          167200    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 777, 200)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 777, 39)           7839      \n",
      "=================================================================\n",
      "Total params: 175,039\n",
      "Trainable params: 175,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(verbose=True):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=NUM_UNITS, return_sequences=True), input_shape=(MAX_SENTENCE_LEN, FEATURE_LEN)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "    model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 320/3696 [=>............................] - ETA: 185s - loss: 1.2607 - acc: 0.1746"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-cb18014d3bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2269\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2271\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ADLxMLDS2017/virt/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "monitor = 'loss'\n",
    "model_filepath = MODEL_PATH + model_{monitor}'.format(monitor=monitor) + '_{epoch:03d}_{loss:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor=monitor, verbose=1, save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor=monitor, patience=2, verbose=0)\n",
    "\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=500, verbose=1, callbacks=[earlystop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sentence2frames_test = dict()\n",
    "read_ark(DATA_PATH + 'fbank/test.ark', sentence2frames_test)\n",
    "read_ark(DATA_PATH + 'mfcc/test.ark', sentence2frames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "CPU times: user 8.34 s, sys: 212 ms, total: 8.55 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "# number of testing sentences\n",
    "print(len(sentence2frames_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 777, 108)\n",
      "CPU times: user 24 ms, sys: 24 ms, total: 48 ms\n",
      "Wall time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def one_hot(n, i):\n",
    "    ans = np.zeros(n)\n",
    "    ans[i] = 1\n",
    "    return ans\n",
    "\n",
    "NUM_SENTENCES_TEST = 592\n",
    "sentences_test = sorted(sentence2frames_test)\n",
    "X_test = np.zeros((NUM_SENTENCES_TEST, MAX_SENTENCE_LEN, FEATURE_LEN))\n",
    "\n",
    "for i, s in enumerate(sentences_test):\n",
    "    frames = sentence2frames_test[s]\n",
    "    X_test[i, :len(frames)] = frames\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 777, 39)\n",
      "(592, 777)\n",
      "CPU times: user 14 s, sys: 568 ms, total: 14.6 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = 'model_val_loss_030_0.2851.h5'\n",
    "model = keras.models.load_model(MODEL_PATH + model_name)\n",
    "y_test_onehot = model.predict(X_test)\n",
    "y_test = np.argmax(y_test_onehot, axis=-1)\n",
    "\n",
    "print(y_test_onehot.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(seq):\n",
    "    L = len(seq)\n",
    "    sil = phone39_int39['sil']\n",
    "    \n",
    "    # trim leading sil\n",
    "    for i in range(L):\n",
    "        if seq[i] != sil:\n",
    "            break\n",
    "    start = i\n",
    "\n",
    "    # trim trailing sil\n",
    "    is_sil = False\n",
    "    for i in range(L - 1, -1, -1):\n",
    "        if is_sil:\n",
    "            if seq[i] != sil:\n",
    "                break\n",
    "        else:\n",
    "            if seq[i] == sil:\n",
    "                is_sil = True\n",
    "    end = i + 1\n",
    "    \n",
    "    trimmed = [phone48_char48[int39_phone39[i]] for i in seq[start:end]]\n",
    "    return''.join([k for k, v in itertools.groupby(trimmed)])\n",
    "\n",
    "y_predicted = list(map(decode, y_test))\n",
    "\n",
    "time = arrow.now('Asia/Taipei').format('YYYYMMDD_HHmmss')\n",
    "prediction_filepath = PREDICTION_PATH + prediction_{time}.csv'.format(time=time)\n",
    "with open(prediction_filepath, 'w') as prediction_f:\n",
    "    prediction_f.write('id,phone_sequence\\n')\n",
    "    for i, sentence in enumerate(sentences_test):\n",
    "        prediction_f.write('{id},{y}\\n'.format(id=sentence, y=y_predicted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = val_acc\n",
    "Epoch 36/200\n",
    "3326/3326 [==============================] - 77s - loss: 0.3140 - acc: 0.2903 - val_loss: 0.3337 - val_acc: 0.2845\n",
    "\n",
    "monitor = val_loss\n",
    "Epoch 23/500\n",
    "3326/3326 [==============================] - 172s - loss: 0.3495 - acc: 0.2870 - val_loss: 0.3622 - val_acc: 0.2925\n",
    "\n",
    "monitor = acc (no validation)\n",
    "Epoch 133/500\n",
    "3696/3696 [==============================] - 109s - loss: 0.2369 - acc: 0.3140"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
